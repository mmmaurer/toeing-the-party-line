import random
from itertools import combinations, product
import argparse

from tqdm import tqdm
import polars as pl

from src.data_processing import (
    DTYPES,
    get_hashtags,
    get_year,
    get_common_hashtags,
    preprocess_parties,
    hashtag_coocurrence
)

# turn off warnings
import warnings
warnings.filterwarnings('ignore')

random.seed(42) # for reproducibility

def load_data(path, has_header=True, 
              separator=",",
              dtypes=DTYPES,
              year_lower_bound=2013,
              year_upper_bound=2016):
    """
    Load data from a csv file and preprocess it.
    The data is filtered for the years between year_lower_bound and
    year_upper_bound to select training data for a specific time
    span.

    Args:
    - path: str, path to the dataset csv file
    - separator: str, separator used in the csv file
    - dtypes: dict, datatypes for the columns
    - year_lower_bound: int, lower bound for the year
    - year_upper_bound: int, upper bound for the year

    Returns:
    - df: polars.DataFrame, preprocessed data
    """
    df = pl.read_csv(path, has_header=has_header,
                     separator=separator, dtypes=dtypes,
                     ignore_errors=True)
    df = preprocess_parties(df)
    df = get_hashtags(df)
    df = get_year(df)
    df = df.filter((df['year'] >= year_lower_bound),
                   (df['year'] <= year_upper_bound))
    df = df.with_columns(
        pl.col('hashtags').list.len().alias('n_hashtags')
    )
    df = df.filter(pl.col('n_hashtags') > 0)
    return df

def generate_datasets(df,
                      n_parties=3,
                      n_tweets=50,
                      n_samples=230):
    """
    Generate a dataset for training and validation.
    The dataset is generated by selecting pairs of sentences
    that contain the same hashtag as positive examples and pairs
    of sentences that contain no co-occurring hashtags as negative
    examples.

    Args:
    - df: polars.DataFrame, preprocessed data
    - n_parties: int, minimum number of parties in whose members' tweets
                 a hashtag should occur in to be considered
    - n_tweets: int, minimum number of tweets a hashtag should occur in
                to be considered
    - n_samples: int, number of samples to generate for each hashtag

    Returns:
    - data: polars.DataFrame, generated dataset
    """

    common_hashtags, _, _ = get_common_hashtags(df, n_parties, n_tweets)
    cooc = hashtag_coocurrence(df, common_hashtags)

    sents1 = []
    sents2 = []
    labels = []

    for hashtag in tqdm(common_hashtags):
        pos_sents = df.filter(pl.col("hashtags").list. \
                              contains(hashtag))["text"].to_list()
        neg_sents = df.filter(
            (pl.col("hashtags").apply(lambda x:
                                      set(x).intersection(cooc[hashtag])). \
                                        apply(len) == 0),
            (~ pl.col("hashtags").list.contains(hashtag))
            )["text"].to_list()
        if len(pos_sents) > 100 and len(neg_sents) > 100:
            pos_samples = 100
            neg_samples = 100
        elif len(pos_sents) > 100:
            pos_samples = 100
            neg_samples = len(neg_sents)
        elif len(neg_sents) > 100:
            pos_samples = len(pos_sents)
            neg_samples = 100
        
        pos_ex = list(combinations(random.sample(pos_sents, pos_samples), 2))
        if len(pos_ex) < n_samples:
            n_pos_samples = len(pos_ex)
        else:
            n_pos_samples = n_samples

        # sample positive examples
        for sent1, sent2 in random.sample(pos_ex, n_pos_samples):
            sents1.append(sent1)
            sents2.append(sent2)
            labels.append(1)
        
        # sample negative examples; sample space is much larger, so
        # we choose a sample of size n_samples
        for sent1, sent2 in random.sample(
            list(
                product(random.sample(pos_sents, pos_samples),
                        random.sample(neg_sents, neg_samples))
                        ),
            n_samples):
            sents1.append(sent1)
            sents2.append(sent2)
            labels.append(0)

    # put data into a DataFrame
    data = pl.DataFrame({
        "sent1": sents1,
        "sent2": sents2,
        "label": labels
    })

    return data

def split_dataset(data):
    """
    Split the dataset into training and validation sets.
    Split size is 80% training and 20% validation.

    Args:
    - data: polars.DataFrame, generated dataset

    Returns:
    - train: polars.DataFrame, training set
    - test: polars.DataFrame, validation set
    """
    # split polars DataFrame into train and test
    # by first shuffling
    data = data.sample(fraction=1, shuffle=True)
    # then splitting
    split = int(0.8 * len(data))
    train = data.slice(0, split)
    test = data.slice(split, len(data))

    return train, test

def save_datasets(train,
                  test,
                  data_path,
                  year,
                  additional_filename=None):
    """
    Save the training and validation sets to disk.

    Args:
    - train: polars.DataFrame, training set
    - test: polars.DataFrame, validation set
    - data_path: str, path to save the datasets
    - year: int, year of the dataset
    - additional_filename: str, additional filename to append to the
                           dataset name
    
    Returns:
    - None
    """
    if additional_filename:
        train.write_csv(data_path + f"train_{year}_{additional_filename}.csv")
        test.write_csv(data_path + f"val_{year}_{additional_filename}.csv")
    else:
        train.write_csv(data_path + f"train_{year}.csv")
        test.write_csv(data_path + f"val_{year}.csv")

if __name__ == '__main__':
    argparser = argparse.ArgumentParser()
    argparser.add_argument('--data_path', type=str, default='./data/',
                           help='Path to the data directory')
    argparser.add_argument('--year_lower_bound', type=int, default=2013,
                           help='Lower bound for the year')
    argparser.add_argument('--year_upper_bound', type=int, default=2016,
                           help='Upper bound for the year')
    argparser.add_argument('--n_parties', type=int, default=3,
                           help='Minimum number of parties for a hashtag'
                                'to be considered')
    argparser.add_argument('--n_tweets', type=int, default=50,
                           help='Minimum number of tweets for a hashtag'
                                'to be considered')
    argparser.add_argument('--n_samples', type=int, default=320,
                           help='Number of samples to generate for each'
                                ' hashtag')
    argparser.add_argument('--year', type=int, default=2017,
                           help='Year of the data')
    argparser.add_argument('--additional_filename', type=str, default='b',
                           help='Additional filename to append to the dataset'
                                'name')
    args = argparser.parse_args()

    df = load_data(args.data_path + f'btw{args.year}_tweets.csv', 
                   year_lower_bound=args.year_lower_bound,
                   year_upper_bound=args.year_upper_bound)
    data = generate_datasets(df,
                             n_parties=args.n_parties,
                             n_hashtags=args.n_tweets,
                             n_samples=args.n_samples)
    train, test = split_dataset(data)
    save_datasets(train, test, args.data_path, args.year)
    print(f"Data for year {args.year} has been saved.")

